New to do list:
    -look into the sb3/cleanRl implementation of smoothing noise for ddpg
    -benchmark qr-ddpg vs regular ddpg
    -then implement qr-sac (use nature supplementary material as a reference)
    -look into SAC actor implementation
    -implement devicing
    Organisation:
        -clean up library
        -implement .train() method and reward logger for all algorithms
        -reformat code with linting (flake/pep8)
        -standardise formatting in utils/model.py
        -standardise replaybuffer usage
        -standardise argument names
        -reformat rewardlogger to give updates based on timesteps not episodes

Thesis:
    -Change training loop to work with trainsteps, not episodes
    -figure out IQL improvements like VDN
    ENGINEERING:
        -implement IQL and VDN
        -experiment with converging on other coop environments
        -create trainer and config files    <- next up
        -create reward tracer with logging etc. X
        -add policy smoothing noise to MATD3 ?

    RESEARCH:
        -look into papers re: transfer learning in MADRL
        -read into paper ivana sent and try integrate it into replay_buffer?
        -consider using different algorithms

    ENVIRONMENT:
        -look into tweaking the simple spread env to reward getting to a landmark
